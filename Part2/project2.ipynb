{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWA Project Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name | Email | UPF uNum |\n",
    "| --- | --- | --- |\n",
    "| Clara Pena | clara.pena01@estudiant.upf.edu | u186416 |\n",
    "| Yuyan Wang | yuyan.wang01@estudiant.upf.edu | u199907 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk import PorterStemmer, word_tokenize, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy.linalg as la\n",
    "import string\n",
    "import textwrap\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117405 entries, 0 to 117404\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        117405 non-null  int64 \n",
      " 1   content   117404 non-null  object\n",
      " 2   date      117405 non-null  object\n",
      " 3   hashtags  116794 non-null  object\n",
      " 4   likes     117405 non-null  int64 \n",
      " 5   retweets  117405 non-null  int64 \n",
      " 6   url       117405 non-null  object\n",
      " 7   language  117405 non-null  object\n",
      " 8   docId     48427 non-null   object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/processed_data.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with those tweets that have a document ID associated with them; that is, the value in the docID column should not be NaN. This is basically taking those tweets in English. Besides that, for simplicity, we're not using the column language anymore, so it can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = df.dropna(subset=['docId']).drop(columns=['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "      <th>docId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1364506167226032128</td>\n",
       "      <td>watch full video farmersprotest nofarmersnofoo...</td>\n",
       "      <td>2021-02-24T09:23:16+00:00</td>\n",
       "      <td>#farmersprotest #NoFarmersNoFood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1364505991887347714</td>\n",
       "      <td>watch full video farmersprotest nofarmersnofood</td>\n",
       "      <td>2021-02-24T09:22:34+00:00</td>\n",
       "      <td>#farmersprotest #NoFarmersNoFood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1364505813834989568</td>\n",
       "      <td>watch full video farmersprotest nofarmersnofood</td>\n",
       "      <td>2021-02-24T09:21:51+00:00</td>\n",
       "      <td>#farmersprotest #NoFarmersNoFood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>doc_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1364505749359976448</td>\n",
       "      <td>anoth farmer malkeet singh mahilpur hoshiarpur...</td>\n",
       "      <td>2021-02-24T09:21:36+00:00</td>\n",
       "      <td>#FarmersProtest</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>https://twitter.com/ShariaActivist/status/1364...</td>\n",
       "      <td>doc_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1364505676375076867</td>\n",
       "      <td>hi tell boss modidontsellfarm thank farmerspro...</td>\n",
       "      <td>2021-02-24T09:21:19+00:00</td>\n",
       "      <td>#ModiDontSellFarmers #FarmersProtest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/KaurDosanjh1979/status/136...</td>\n",
       "      <td>doc_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1364505511073300481</td>\n",
       "      <td>watch full video farmersprotest nofarmersnofood</td>\n",
       "      <td>2021-02-24T09:20:39+00:00</td>\n",
       "      <td>#farmersprotest #NoFarmersNoFood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>doc_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1364505452134817795</td>\n",
       "      <td>despit increas tax petroldiesel must increas t...</td>\n",
       "      <td>2021-02-24T09:20:25+00:00</td>\n",
       "      <td>#taxes #petrolDiesel #taxes #alcohol #cigarett...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/Satende09192805/status/136...</td>\n",
       "      <td>doc_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1364505443997937669</td>\n",
       "      <td>mockeri menac sedit charg farmersprotest</td>\n",
       "      <td>2021-02-24T09:20:23+00:00</td>\n",
       "      <td>#sedition #FarmersProtest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/algo_121/status/1364505443...</td>\n",
       "      <td>doc_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1364505314586951680</td>\n",
       "      <td>watch full video farmersprotest nofarmersnofood</td>\n",
       "      <td>2021-02-24T09:19:52+00:00</td>\n",
       "      <td>#farmersprotest #NoFarmersNoFood</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>doc_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1364505255946379268</td>\n",
       "      <td>left hear modi lol farmersprotest</td>\n",
       "      <td>2021-02-24T09:19:38+00:00</td>\n",
       "      <td>#FarmersProtest</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/kdhanjal12/status/13645052...</td>\n",
       "      <td>doc_11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                            content  \\\n",
       "1   1364506167226032128  watch full video farmersprotest nofarmersnofoo...   \n",
       "6   1364505991887347714    watch full video farmersprotest nofarmersnofood   \n",
       "9   1364505813834989568    watch full video farmersprotest nofarmersnofood   \n",
       "10  1364505749359976448  anoth farmer malkeet singh mahilpur hoshiarpur...   \n",
       "14  1364505676375076867  hi tell boss modidontsellfarm thank farmerspro...   \n",
       "16  1364505511073300481    watch full video farmersprotest nofarmersnofood   \n",
       "18  1364505452134817795  despit increas tax petroldiesel must increas t...   \n",
       "20  1364505443997937669           mockeri menac sedit charg farmersprotest   \n",
       "25  1364505314586951680    watch full video farmersprotest nofarmersnofood   \n",
       "26  1364505255946379268                  left hear modi lol farmersprotest   \n",
       "\n",
       "                         date  \\\n",
       "1   2021-02-24T09:23:16+00:00   \n",
       "6   2021-02-24T09:22:34+00:00   \n",
       "9   2021-02-24T09:21:51+00:00   \n",
       "10  2021-02-24T09:21:36+00:00   \n",
       "14  2021-02-24T09:21:19+00:00   \n",
       "16  2021-02-24T09:20:39+00:00   \n",
       "18  2021-02-24T09:20:25+00:00   \n",
       "20  2021-02-24T09:20:23+00:00   \n",
       "25  2021-02-24T09:19:52+00:00   \n",
       "26  2021-02-24T09:19:38+00:00   \n",
       "\n",
       "                                             hashtags  likes  retweets  \\\n",
       "1                    #farmersprotest #NoFarmersNoFood      0         0   \n",
       "6                    #farmersprotest #NoFarmersNoFood      0         0   \n",
       "9                    #farmersprotest #NoFarmersNoFood      0         0   \n",
       "10                                    #FarmersProtest      3         3   \n",
       "14               #ModiDontSellFarmers #FarmersProtest      0         0   \n",
       "16                   #farmersprotest #NoFarmersNoFood      0         0   \n",
       "18  #taxes #petrolDiesel #taxes #alcohol #cigarett...      1         1   \n",
       "20                          #sedition #FarmersProtest      0         0   \n",
       "25                   #farmersprotest #NoFarmersNoFood      0         0   \n",
       "26                                    #FarmersProtest      1         0   \n",
       "\n",
       "                                                  url   docId  \n",
       "1   https://twitter.com/anmoldhaliwal/status/13645...   doc_2  \n",
       "6   https://twitter.com/anmoldhaliwal/status/13645...   doc_3  \n",
       "9   https://twitter.com/anmoldhaliwal/status/13645...   doc_4  \n",
       "10  https://twitter.com/ShariaActivist/status/1364...   doc_5  \n",
       "14  https://twitter.com/KaurDosanjh1979/status/136...   doc_6  \n",
       "16  https://twitter.com/anmoldhaliwal/status/13645...   doc_7  \n",
       "18  https://twitter.com/Satende09192805/status/136...   doc_8  \n",
       "20  https://twitter.com/algo_121/status/1364505443...   doc_9  \n",
       "25  https://twitter.com/anmoldhaliwal/status/13645...  doc_10  \n",
       "26  https://twitter.com/kdhanjal12/status/13645052...  doc_11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 48427 entries, 1 to 117404\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        48427 non-null  int64 \n",
      " 1   content   48427 non-null  object\n",
      " 2   date      48427 non-null  object\n",
      " 3   hashtags  48153 non-null  object\n",
      " 4   likes     48427 non-null  int64 \n",
      " 5   retweets  48427 non-null  int64 \n",
      " 6   url       48427 non-null  object\n",
      " 7   docId     48427 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(df):\n",
    "    index = defaultdict(list)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = row['docId']\n",
    "        terms = row['content'].split()\n",
    "        \n",
    "        current_page_index = {}\n",
    "        for position, term in enumerate(terms):\n",
    "            if term in current_page_index:\n",
    "                current_page_index[term][1].append(position)\n",
    "            else:\n",
    "                current_page_index[term] = [doc_id, array('I', [position])]  # 'I' for array of unsigned ints.\n",
    "\n",
    "        # Merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30477\n"
     ]
    }
   ],
   "source": [
    "print(len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    # stemmer = PorterStemmer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.lower()\n",
    "\n",
    "    # Handle contractions by removing possessive endings and common contractions\n",
    "    line = re.sub(r\"\\b(\\w+)'s\\b\", r'\\1', line)  # Changes \"people's\" to \"people\"\n",
    "    line = re.sub(r\"\\b(\\w+)n't\\b\", r'\\1 not', line)  # Changes \"isn't\" to \"is not\"\n",
    "    line = re.sub(r\"\\b(\\w+)'ll\\b\", r'\\1 will', line)  # Changes \"I'll\" to \"I will\"\n",
    "    line = re.sub(r\"\\b(\\w+)'d\\b\", r'\\1 would', line)  # Changes \"I'd\" to \"I would\"\n",
    "    line = re.sub(r\"\\b(\\w+)'re\\b\", r'\\1 are', line)  # Changes \"you're\" to \"you are\"\n",
    "    line = re.sub(r\"\\b(\\w+)'ve\\b\", r'\\1 have', line)  # Changes \"I've\" to \"I have\"\n",
    "\n",
    "    line = line.split()\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    line = [w.translate(table) for w in line]\n",
    "    line = [w for w in line if w not in stop_words]\n",
    "    line = [stemmer.stem(w) for w in line] \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    query = build_terms(query)  # Normalize and tokenize the query.\n",
    "    docs = None  # Initialize docs as None to handle the intersection.\n",
    "\n",
    "    for term in query:\n",
    "        try:\n",
    "            # Extract the document IDs for the term.\n",
    "            term_docs = set([posting[0] for posting in index[term]])\n",
    "\n",
    "            if docs is None:\n",
    "                # Initialize docs with the set of document IDs for the first term.\n",
    "                docs = term_docs\n",
    "            else:\n",
    "                # Intersect the sets of document IDs.\n",
    "                docs = docs.intersection(term_docs)\n",
    "        except KeyError:\n",
    "            # If the term is not in the index, return an empty list because no documents can satisfy the query.\n",
    "            return []\n",
    "\n",
    "    if docs is None:\n",
    "        return []  # If no terms were processed, return an empty list.\n",
    "    else:\n",
    "        return list(docs)  # Convert the set to a list before returning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('farmersprotest', 50272), ('farmer', 17421), ('india', 7724), ('support', 6004), ('protest', 4787), ('amp', 4728), ('right', 3594), ('peopl', 3526), ('modi', 3113), ('indian', 3008), ('govern', 2753), ('bjp', 2649), ('law', 2570), ('releasedetainedfarm', 2432), ('govt', 2338), ('stand', 2207), ('farmersmakeindia', 2133), ('indiabeingsilenc', 2133), ('thank', 2129), ('farm', 2066)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the top n terms in the DataFrame for the specified column.\n",
    "def get_top_terms(df, column='content', top_n=10):\n",
    "    text = ' '.join(df[column].dropna())  # Combine all text and convert to lower case.\n",
    "    words = text.split()\n",
    "    # Get a count of all words\n",
    "    word_count = Counter(words)\n",
    "    # Return the most common words\n",
    "    return word_count.most_common(top_n)\n",
    "\n",
    "top_terms = get_top_terms(tweets_df, column='content', top_n=20)\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\t“India protest”: This query aligns with the high frequency of terms like ‘india’ (7724 occurrences) and ‘protest’ (4787 occurrences). It targets the significant discussion around protests in India, capturing a broad yet significant topic within your dataset. This is crucial for understanding general sentiments or events related to protests in the region.\n",
    "2.\t“support farmers”: Given the prevalence of terms such as ‘farmersprotest’ (50272 occurrences) and ‘support’ (6004 occurrences), this query is highly relevant. It specifically addresses the widespread discourse on supporting farmers, likely related to agricultural policies or farmer welfare, which seems to be a prominent issue in your data.\n",
    "3.\t“Modi shame”: This query is crafted around the high occurrence of ‘modi’ (3113 occurrences). Including a sentiment or descriptive term like ‘shame’ might target specific discussions or criticisms related to policies or actions associated with Modi, the Prime Minister of India, offering insights into public sentiment regarding his administration.\n",
    "4.\t“BJP party”: With ‘bjp’ being mentioned 2649 times, focusing on the political party directly allows for an analysis of content specifically related to the Bharatiya Janata Party. This could reveal discussions on political activities, policies, or public opinions directly connected to the party’s actions and governance.\n",
    "5.\t“human rights violated”: Although ‘right’ appears 3594 times, coupling it with ‘violated’ expands the context to human rights issues. This query is intended to explore discussions or reports on human rights violations, a topic of critical importance that may encompass various aspects of social and political discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 951 for the searched query 'India protest':\n",
      "\n",
      "docId = doc_45342\n",
      "docId = doc_11845\n",
      "docId = doc_46574\n",
      "docId = doc_46761\n",
      "docId = doc_44458\n",
      "docId = doc_27337\n",
      "docId = doc_40146\n",
      "docId = doc_27301\n",
      "docId = doc_27472\n",
      "docId = doc_7237\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 3197 for the searched query 'support farmers':\n",
      "\n",
      "docId = doc_7211\n",
      "docId = doc_7052\n",
      "docId = doc_42028\n",
      "docId = doc_47335\n",
      "docId = doc_17669\n",
      "docId = doc_35974\n",
      "docId = doc_39136\n",
      "docId = doc_11205\n",
      "docId = doc_17527\n",
      "docId = doc_43582\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 193 for the searched query 'Modi shame':\n",
      "\n",
      "docId = doc_7101\n",
      "docId = doc_20968\n",
      "docId = doc_29421\n",
      "docId = doc_31277\n",
      "docId = doc_37129\n",
      "docId = doc_36097\n",
      "docId = doc_47021\n",
      "docId = doc_13115\n",
      "docId = doc_8528\n",
      "docId = doc_33918\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 128 for the searched query 'BJP party':\n",
      "\n",
      "docId = doc_24118\n",
      "docId = doc_4669\n",
      "docId = doc_33768\n",
      "docId = doc_46569\n",
      "docId = doc_2917\n",
      "docId = doc_20542\n",
      "docId = doc_10013\n",
      "docId = doc_2934\n",
      "docId = doc_40858\n",
      "docId = doc_23877\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 254 for the searched query 'human rights violated':\n",
      "\n",
      "docId = doc_14008\n",
      "docId = doc_13293\n",
      "docId = doc_16493\n",
      "docId = doc_33932\n",
      "docId = doc_31537\n",
      "docId = doc_13273\n",
      "docId = doc_13353\n",
      "docId = doc_13369\n",
      "docId = doc_18515\n",
      "docId = doc_7150\n"
     ]
    }
   ],
   "source": [
    "def simulate_search(queries, index):\n",
    "    for query in queries:\n",
    "        docs = search(query, index)\n",
    "        top = 10  # Number of results to display\n",
    "        num_results = len(docs)\n",
    "        \n",
    "        print(\"\\n======================\\nSample of {} results out of {} for the searched query '{}':\\n\".format(min(top, num_results), num_results, query))\n",
    "        for d_id in docs[:top]:\n",
    "            print(\"docId = {}\".format(d_id))\n",
    "\n",
    "# List of queries to be processed\n",
    "queries = [\n",
    "    \"India protest\",\n",
    "    \"support farmers\",\n",
    "    \"Modi shame\",\n",
    "    \"BJP party\",\n",
    "    \"human rights violated\"\n",
    "]\n",
    "\n",
    "simulate_search(queries, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(dataframe):\n",
    "    # num_documents = len(df)\n",
    "    num_documents = dataframe['docId'].nunique()\n",
    "    index = defaultdict(list)\n",
    "    # tf = defaultdict(dict)  # Normalized term frequencies of terms in documents\n",
    "    tf = defaultdict(list)\n",
    "    df = defaultdict(int)  # Document frequencies of terms\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        doc_id = row['docId']\n",
    "        terms = row['content'].split()\n",
    "        \n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            if term in current_page_index:\n",
    "                # Append the position to the corresponding list in the array\n",
    "                current_page_index[term][1].append(position)\n",
    "            else:\n",
    "                # Initialize the list with page_id and a new array\n",
    "                current_page_index[term] = [doc_id, array('I', [position])]\n",
    "\n",
    "        # Calculate the norm for the terms in the document\n",
    "        norm = math.sqrt(sum(len(positions[1])**2 for positions in current_page_index.values()))\n",
    "\n",
    "        # calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "        \n",
    "        # Merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    for term in df:\n",
    "        idf[term] = math.log(num_documents / (1 + df[term]))  # Smoothing by adding 1 to denominator\n",
    "\n",
    "    return index, tf, df, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, tf, df, idf = create_index_tfidf(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf):\n",
    "    doc_vectors = defaultdict(lambda: np.zeros(len(terms)))\n",
    "    query_vector = np.zeros(len(terms))\n",
    "    query_terms_count = Counter(terms)\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    # Compute the tf-idf for the query vector\n",
    "    for term_index, term in enumerate(terms):\n",
    "        if term in idf:\n",
    "            query_vector[term_index] = query_terms_count[term] / query_norm * idf[term]\n",
    "            if term not in index:\n",
    "                continue\n",
    "    \n",
    "            for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "                if doc in docs:\n",
    "                    doc_vectors[doc][term_index] = tf[term][doc_index] * idf[term]  # T\n",
    "            \n",
    "    # Calculate the score of each doc using cosine similarity (dot product of normalized vectors)\n",
    "    doc_scores = [[np.dot(cur_doc_vec, query_vector), doc] for doc, cur_doc_vec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    # print(doc_scores)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "    else:\n",
    "        return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index, idf, tf):\n",
    "    query = build_terms(query)\n",
    "    # print(query)\n",
    "    docs = None  # Initialize to None to handle the first term's document set initialization\n",
    "\n",
    "    for term in query:\n",
    "        if term in index:\n",
    "            term_docs = set([posting[0] for posting in index[term]])  # Collect all document IDs containing this term\n",
    "            if docs is None:\n",
    "                docs = term_docs\n",
    "            else:\n",
    "                docs = docs.intersection(term_docs)  # Intersection with the accumulated set of documents\n",
    "        else:\n",
    "            return []  # If any term is not found, the intersection is empty\n",
    "\n",
    "    if docs is None:\n",
    "        return []  # No terms found, return empty list\n",
    "\n",
    "    docs = list(docs)  # Convert set to list if necessary\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf)  # Rank the documents based on the relevance\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_search_tf_idf(queries, index, idf, tf):\n",
    "    for query in queries:\n",
    "        ranked_docs = search_tf_idf(query, index, idf, tf)\n",
    "        top = 10  # Number of results to display\n",
    "        num_results = len(ranked_docs)\n",
    "        \n",
    "        print(\"\\n======================\\nSample of {} results out of {} for the searched query '{}':\\n\".format(min(top, num_results), num_results, query))\n",
    "        for d_id in ranked_docs[:top]:\n",
    "            print(\"docId = {}\".format(d_id))\n",
    "\n",
    "# List of queries to be processed\n",
    "queries = [\n",
    "    \"India protest\",\n",
    "    \"support farmers\",\n",
    "    \"Modi shame\",\n",
    "    \"BJP party\",\n",
    "    \"human rights violated\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 10 results out of 951 for the searched query 'India protest':\n",
      "\n",
      "docId = doc_17093\n",
      "docId = doc_17097\n",
      "docId = doc_38842\n",
      "docId = doc_40320\n",
      "docId = doc_32847\n",
      "docId = doc_445\n",
      "docId = doc_27693\n",
      "docId = doc_46574\n",
      "docId = doc_13552\n",
      "docId = doc_29037\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 3197 for the searched query 'support farmers':\n",
      "\n",
      "docId = doc_38864\n",
      "docId = doc_43187\n",
      "docId = doc_31878\n",
      "docId = doc_47382\n",
      "docId = doc_47396\n",
      "docId = doc_47423\n",
      "docId = doc_30210\n",
      "docId = doc_24459\n",
      "docId = doc_3699\n",
      "docId = doc_4430\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 193 for the searched query 'Modi shame':\n",
      "\n",
      "docId = doc_7810\n",
      "docId = doc_37129\n",
      "docId = doc_38556\n",
      "docId = doc_39102\n",
      "docId = doc_41693\n",
      "docId = doc_42459\n",
      "docId = doc_42461\n",
      "docId = doc_42463\n",
      "docId = doc_45856\n",
      "docId = doc_45858\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 128 for the searched query 'BJP party':\n",
      "\n",
      "docId = doc_39703\n",
      "docId = doc_14912\n",
      "docId = doc_35686\n",
      "docId = doc_19862\n",
      "docId = doc_27074\n",
      "docId = doc_43419\n",
      "docId = doc_29032\n",
      "docId = doc_38429\n",
      "docId = doc_42112\n",
      "docId = doc_4683\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 254 for the searched query 'human rights violated':\n",
      "\n",
      "docId = doc_5643\n",
      "docId = doc_14008\n",
      "docId = doc_23717\n",
      "docId = doc_30836\n",
      "docId = doc_46246\n",
      "docId = doc_26878\n",
      "docId = doc_7150\n",
      "docId = doc_14648\n",
      "docId = doc_16758\n",
      "docId = doc_19102\n"
     ]
    }
   ],
   "source": [
    "simulate_search_tf_idf(queries, index, idf, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_gt = pd.read_csv('./data/evaluation_gt.csv', sep=';')\n",
    "df_eva = pd.merge(tweets_df, evaluation_gt, on='docId', how='left')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "query_1_relevant = (df_eva[(df_eva['query_id'] == 1) & (df_eva['label'] == 1)])['docId'].unique()\n",
    "query_1_not_relevant = (df_eva[(df_eva['query_id'] == 1) & (df_eva['label'] == 0)])['docId'].unique()\n",
    "\n",
    "query_2_relevant = (df_eva[(df_eva['query_id'] == 2) & (df_eva['label'] == 1)])['docId'].unique()\n",
    "query_2_not_relevant = (df_eva[(df_eva['query_id'] == 2) & (df_eva['label'] == 0)])['docId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "# Keep in mind that for the evaluation part we will be using only the subset of documents that are being defined in the evaluation_gt.csv\n",
    "df_subset_documents = df_eva[df_eva['query_id'].notnull()]\n",
    "subset_documents = df_subset_documents['docId'].unique()\n",
    "print(len(subset_documents))\n",
    "\n",
    "index, tf, df, idf = create_index_tfidf(df_subset_documents)\n",
    "query_1_results = search_tf_idf(\"people's rights\", index, idf, tf)\n",
    "query_2_results = search_tf_idf(\"Indian government\", index, idf, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Files Query 1 (subset): ['doc_1047' 'doc_2100' 'doc_3287' 'doc_3474' 'doc_3570' 'doc_4053'\n",
      " 'doc_5480' 'doc_5512' 'doc_5751' 'doc_6477' 'doc_8066' 'doc_9696'\n",
      " 'doc_9850' 'doc_9937' 'doc_10048']\n",
      "\n",
      "Our Obtained Results Query 1: ['doc_4053', 'doc_9850', 'doc_9696', 'doc_6477', 'doc_2100', 'doc_2732', 'doc_8819',\n",
      "'doc_43341', 'doc_8066', 'doc_5480', 'doc_3474', 'doc_43540', 'doc_5751', 'doc_10048',\n",
      "'doc_5512', 'doc_1047', 'doc_3287', 'doc_3570', 'doc_9937']\n",
      "\n",
      "Ground Truth Files Query 2 (subset): ['doc_103' 'doc_1566' 'doc_1651' 'doc_1666' 'doc_1785' 'doc_2528'\n",
      " 'doc_2653' 'doc_3005' 'doc_3076' 'doc_3116' 'doc_3646' 'doc_3682'\n",
      " 'doc_3927' 'doc_4176' 'doc_4304']\n",
      "\n",
      "Our Obtained Results Query 2: ['doc_3116', 'doc_103', 'doc_1566', 'doc_3076', 'doc_3682', 'doc_3646', 'doc_2653',\n",
      "'doc_3927', 'doc_1666', 'doc_3005', 'doc_1651', 'doc_4304', 'doc_1785', 'doc_4176',\n",
      "'doc_2528']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_wrapped(title, data):\n",
    "    wrapper = textwrap.TextWrapper(width=90)\n",
    "    wrapped_text = wrapper.fill(str(data))\n",
    "    print(f\"{title} {wrapped_text}\\n\")\n",
    "\n",
    "print(f\"Ground Truth Files Query 1 (subset): {query_1_relevant}\\n\")\n",
    "print_wrapped(\"Our Obtained Results Query 1:\", query_1_results)\n",
    "\n",
    "print(f\"Ground Truth Files Query 2 (subset): {query_2_relevant}\\n\")\n",
    "print_wrapped(\"Our Obtained Results Query 2:\", query_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Ground Truth Files Query 1 (subset): ['doc_1047' 'doc_2100' 'doc_3287' 'doc_3474' 'doc_3570' 'doc_4053'\n",
      " 'doc_5480' 'doc_5512' 'doc_5751' 'doc_6477' 'doc_8066' 'doc_9696'\n",
      " 'doc_9850' 'doc_9937' 'doc_10048']\n",
      "\n",
      "Our Obtained Results Query 1: ['doc_4053', 'doc_9850', 'doc_9696', 'doc_6477', 'doc_2732', 'doc_8819', 'doc_8066',\n",
      "'doc_5480', 'doc_2100', 'doc_5751', 'doc_10048', 'doc_5512', 'doc_3474', 'doc_1047',\n",
      "'doc_3287', 'doc_3570', 'doc_9937']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_subset_documents_query_1 = df_eva[df_eva['query_id'] == 1]\n",
    "print(len(df_subset_documents_query_1))\n",
    "index, tf, df, idf = create_index_tfidf(df_subset_documents_query_1)\n",
    "query_1_results = search_tf_idf(\"people's rights\", index, idf, tf)\n",
    "\n",
    "print(f\"Ground Truth Files Query 1 (subset): {query_1_relevant}\\n\")\n",
    "print_wrapped(\"Our Obtained Results Query 1:\", query_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Ground Truth Files Query 2 (subset): ['doc_103' 'doc_1566' 'doc_1651' 'doc_1666' 'doc_1785' 'doc_2528'\n",
      " 'doc_2653' 'doc_3005' 'doc_3076' 'doc_3116' 'doc_3646' 'doc_3682'\n",
      " 'doc_3927' 'doc_4176' 'doc_4304']\n",
      "\n",
      "Our Obtained Results Query 2: ['doc_3116', 'doc_103', 'doc_3076', 'doc_3682', 'doc_3646', 'doc_1566', 'doc_2653',\n",
      "'doc_1666', 'doc_3005', 'doc_1651', 'doc_4304', 'doc_1785', 'doc_4176', 'doc_3927',\n",
      "'doc_2528']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_subset_documents_query_2 = df_eva[df_eva['query_id'] == 2]\n",
    "print(len(df_subset_documents_query_2))\n",
    "index, tf, df, idf = create_index_tfidf(df_subset_documents_query_2)\n",
    "query_2_results = search_tf_idf(\"Indian government\", index, idf, tf)\n",
    "\n",
    "print(f\"Ground Truth Files Query 2 (subset): {query_2_relevant}\\n\")\n",
    "print_wrapped(\"Our Obtained Results Query 2:\", query_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "      <th>docId</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1364505255946379268</td>\n",
       "      <td>left hear modi lol farmersprotest</td>\n",
       "      <td>2021-02-24T09:19:38+00:00</td>\n",
       "      <td>#FarmersProtest</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/kdhanjal12/status/1364505255946379268</td>\n",
       "      <td>doc_11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1364504281618001921</td>\n",
       "      <td>know tiger wood accid what go ten thousand indian farmer protest farmersprotest</td>\n",
       "      <td>2021-02-24T09:15:46+00:00</td>\n",
       "      <td>#FarmersProtest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/GregMitchell62/status/1364504281618001921</td>\n",
       "      <td>doc_17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  \\\n",
       "9   1364505255946379268   \n",
       "15  1364504281618001921   \n",
       "\n",
       "                                                                            content  \\\n",
       "9                                                 left hear modi lol farmersprotest   \n",
       "15  know tiger wood accid what go ten thousand indian farmer protest farmersprotest   \n",
       "\n",
       "                         date         hashtags  likes  retweets  \\\n",
       "9   2021-02-24T09:19:38+00:00  #FarmersProtest      1         0   \n",
       "15  2021-02-24T09:15:46+00:00  #FarmersProtest      0         0   \n",
       "\n",
       "                                                              url   docId  \\\n",
       "9       https://twitter.com/kdhanjal12/status/1364505255946379268  doc_11   \n",
       "15  https://twitter.com/GregMitchell62/status/1364504281618001921  doc_17   \n",
       "\n",
       "    query_id  label  \n",
       "9        3.0    0.0  \n",
       "15       1.0    1.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personalized_eva_gt = pd.read_csv('./data/personalized_evaluation_gt.csv', sep=';')\n",
    "df_personalized_eva = pd.merge(tweets_df, personalized_eva_gt, on='docId', how='left')\n",
    "df_personalized_eva = df_personalized_eva[df_personalized_eva['query_id'].notnull()]\n",
    "df_personalized_eva.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to hold relevant documents and results\n",
    "personalized_query_text = {}\n",
    "personalized_query_relevant = {}\n",
    "personalized_query_results = {}\n",
    "\n",
    "query_texts = [\"India protest\", \"support farmers\", \"Modi shame\", \"BJP party\", \"human rights violated\"]\n",
    "\n",
    "# Populate the dictionaries\n",
    "for i, query_text in enumerate(query_texts, start=1):\n",
    "    personalized_query_text[i] = query_texts[i-1]\n",
    "    df_query = df_personalized_eva[df_personalized_eva['query_id'] == i]\n",
    "    personalized_query_relevant[i] = df_query[df_query['label'] == 1]['docId'].unique()\n",
    "    index, tf, df, idf = create_index_tfidf(df_query)\n",
    "    personalized_query_results[i] = search_tf_idf(query_text, index, idf, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind here that we are computing the Binary Relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ground_truth, results, K=10):\n",
    "    top_k_results = results[:K]\n",
    "    # Calculate the number of relevant documents in the top K results\n",
    "    relevant_documents = [doc for doc in top_k_results if doc in ground_truth]\n",
    "    precision = len(relevant_documents) / K if K > 0 else 0\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 Precision@5: 0.8000\n",
      "Query 1 Precision@10: 0.8000\n",
      "Query 1 Precision@15: 0.8667\n",
      "Query 1 Precision@17: 0.8824\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_1_results)])))\n",
    "for K in K_values:\n",
    "    precision = precision_at_k(query_1_relevant, query_1_results, K)\n",
    "    print(f\"Query 1 Precision@{K}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 Precision@5: 1.0000\n",
      "Query 2 Precision@10: 1.0000\n",
      "Query 2 Precision@15: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_2_results)])))\n",
    "for K in K_values:\n",
    "    precision = precision_at_k(query_2_relevant, query_2_results, K)\n",
    "    print(f\"Query 2 Precision@{K}: {precision:.4f}\")\n",
    "\n",
    "# TODO: Check following markdown text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider that the precision should be computed for each query separately. Besides that, since we're analyzing binary relevance, it makes more sense to take as final metric the precision done at level that is the actual length of the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Query 1 - India protest             - Precision@5  : 1.0000\n",
      "Personalized Query 1 - India protest             - Precision@10 : 0.5000\n",
      "Personalized Query 1 - India protest             - Precision@15 : 0.3333\n",
      "\n",
      "Personalized Query 2 - support farmers           - Precision@5  : 1.0000\n",
      "Personalized Query 2 - support farmers           - Precision@10 : 1.0000\n",
      "Personalized Query 2 - support farmers           - Precision@15 : 0.6667\n",
      "\n",
      "Personalized Query 3 - Modi shame                - Precision@5  : 1.0000\n",
      "Personalized Query 3 - Modi shame                - Precision@10 : 1.0000\n",
      "Personalized Query 3 - Modi shame                - Precision@15 : 0.6667\n",
      "\n",
      "Personalized Query 4 - BJP party                 - Precision@5  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - Precision@9  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - Precision@10 : 0.9000\n",
      "Personalized Query 4 - BJP party                 - Precision@15 : 0.6000\n",
      "\n",
      "Personalized Query 5 - human rights violated     - Precision@5  : 1.0000\n",
      "Personalized Query 5 - human rights violated     - Precision@10 : 1.0000\n",
      "Personalized Query 5 - human rights violated     - Precision@15 : 0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, query_text in enumerate(query_texts, start=1):\n",
    "    # Retrieve relevant documents and results from the dictionaries\n",
    "    relevant_docs = personalized_query_relevant[i]\n",
    "    results = personalized_query_results[i]\n",
    "    \n",
    "    K_values = list(sorted(set([5, 10, 15, len(results)])))\n",
    "    \n",
    "    for K in K_values:\n",
    "        precision = precision_at_k(relevant_docs, results, K)\n",
    "        print(f\"Personalized Query {i} - {query_text:<25} - Precision@{K:<3}: {precision:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall@K (R@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(ground_truth, results, K=10):\n",
    "    if K > len(results): K = len(results)\n",
    "    top_k_results = results[:K]\n",
    "    relevant_documents_retrieved = sum(1 for doc in top_k_results if doc in ground_truth)\n",
    "    total_relevant_documents = len(ground_truth)\n",
    "    if total_relevant_documents == 0: return 0\n",
    "    recall = relevant_documents_retrieved / total_relevant_documents\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 Recall@5: 0.2667\n",
      "Query 1 Recall@10: 0.5333\n",
      "Query 1 Recall@15: 0.8667\n",
      "Query 1 Recall@17: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_1_results)])))\n",
    "for K in K_values:\n",
    "    recall_value = recall_at_k(query_1_relevant, query_1_results, K)\n",
    "    print(f\"Query 1 Recall@{K}: {recall_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 Recall@5: 0.3333\n",
      "Query 2 Recall@10: 0.6667\n",
      "Query 2 Recall@15: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_2_results)])))\n",
    "for K in K_values:\n",
    "    recall_value = recall_at_k(query_2_relevant, query_2_results, K)\n",
    "    print(f\"Query 2 Recall@{K}: {recall_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Query 1 - India protest             - Recall@5  : 0.5000\n",
      "Personalized Query 1 - India protest             - Recall@10 : 0.5000\n",
      "Personalized Query 1 - India protest             - Recall@15 : 0.5000\n",
      "\n",
      "Personalized Query 2 - support farmers           - Recall@5  : 0.5000\n",
      "Personalized Query 2 - support farmers           - Recall@10 : 1.0000\n",
      "Personalized Query 2 - support farmers           - Recall@15 : 1.0000\n",
      "\n",
      "Personalized Query 3 - Modi shame                - Recall@5  : 0.5000\n",
      "Personalized Query 3 - Modi shame                - Recall@10 : 1.0000\n",
      "Personalized Query 3 - Modi shame                - Recall@15 : 1.0000\n",
      "\n",
      "Personalized Query 4 - BJP party                 - Recall@5  : 0.5556\n",
      "Personalized Query 4 - BJP party                 - Recall@9  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - Recall@10 : 1.0000\n",
      "Personalized Query 4 - BJP party                 - Recall@15 : 1.0000\n",
      "\n",
      "Personalized Query 5 - human rights violated     - Recall@5  : 0.5000\n",
      "Personalized Query 5 - human rights violated     - Recall@10 : 1.0000\n",
      "Personalized Query 5 - human rights violated     - Recall@15 : 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, query_text in enumerate(query_texts, start=1):\n",
    "    relevant_docs = personalized_query_relevant[i]\n",
    "    results = personalized_query_results[i]\n",
    "    \n",
    "    K_values = list(sorted(set([5, 10, 15, len(results)])))\n",
    "    \n",
    "    for K in K_values:\n",
    "        recall_value = recall_at_k(relevant_docs, results, K)\n",
    "        print(f\"Personalized Query {i} - {query_text:<25} - Recall@{K:<3}: {recall_value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision@K (P@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_at_k(ground_truth, results, K=None):\n",
    "    if K is None: K = len(results)\n",
    "\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    relevant_documents_retrieved = 0\n",
    "    cumulative_precision = 0.0\n",
    "\n",
    "    # Iterate over the list of results up to K\n",
    "    for i, doc_id in enumerate(results[:K]):\n",
    "        if doc_id in ground_truth_set:\n",
    "            relevant_documents_retrieved += 1\n",
    "            precision_at_i = relevant_documents_retrieved / (i + 1)\n",
    "            cumulative_precision += precision_at_i\n",
    "\n",
    "    # Calculate average precision\n",
    "    total_relevant = len(ground_truth_set)\n",
    "    if total_relevant > 0:\n",
    "        average_precision = cumulative_precision / total_relevant\n",
    "    else:\n",
    "        average_precision = 0\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 Average Precision@5: 0.2667\n",
      "Query 1 Average Precision@10: 0.4695\n",
      "Query 1 Average Precision@15: 0.7509\n",
      "Query 1 Average Precision@17: 0.8681\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_1_results)])))\n",
    "for K in K_values:\n",
    "    precision = average_precision_at_k(query_1_relevant, query_1_results, K)\n",
    "    print(f\"Query 1 Average Precision@{K}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 Average Precision@5: 0.3333\n",
      "Query 2 Average Precision@10: 0.6667\n",
      "Query 2 Average Precision@15: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_2_results)])))\n",
    "for K in K_values:\n",
    "    precision = average_precision_at_k(query_2_relevant, query_2_results, K)\n",
    "    print(f\"Query 2 Average Precision@{K}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Query 1 - India protest             - Average Precision@5  : 0.5000\n",
      "Personalized Query 1 - India protest             - Average Precision@10 : 0.5000\n",
      "Personalized Query 1 - India protest             - Average Precision@15 : 0.5000\n",
      "\n",
      "Personalized Query 2 - support farmers           - Average Precision@5  : 0.5000\n",
      "Personalized Query 2 - support farmers           - Average Precision@10 : 1.0000\n",
      "Personalized Query 2 - support farmers           - Average Precision@15 : 1.0000\n",
      "\n",
      "Personalized Query 3 - Modi shame                - Average Precision@5  : 0.5000\n",
      "Personalized Query 3 - Modi shame                - Average Precision@10 : 1.0000\n",
      "Personalized Query 3 - Modi shame                - Average Precision@15 : 1.0000\n",
      "\n",
      "Personalized Query 4 - BJP party                 - Average Precision@5  : 0.5556\n",
      "Personalized Query 4 - BJP party                 - Average Precision@9  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - Average Precision@10 : 1.0000\n",
      "Personalized Query 4 - BJP party                 - Average Precision@15 : 1.0000\n",
      "\n",
      "Personalized Query 5 - human rights violated     - Average Precision@5  : 0.5000\n",
      "Personalized Query 5 - human rights violated     - Average Precision@10 : 1.0000\n",
      "Personalized Query 5 - human rights violated     - Average Precision@15 : 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check if results make sense\n",
    "\n",
    "for i, query_text in enumerate(query_texts, start=1):\n",
    "    relevant_docs = personalized_query_relevant[i]\n",
    "    results = personalized_query_results[i]\n",
    "    \n",
    "    K_values = list(sorted(set([5, 10, 15, len(results)])))\n",
    "    for K in K_values:\n",
    "        precision = average_precision_at_k(relevant_docs, results, K)\n",
    "        print(f\"Personalized Query {i} - {query_text:<25} - Average Precision@{K:<3}: {precision:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k(ground_truth, results, K=None):\n",
    "    if K is None: K = len(results)\n",
    "\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    relevant_documents_retrieved = 0\n",
    "    results_considered = results[:K]\n",
    "\n",
    "    # We can be also using defined precision and recall at k functions\n",
    "    # Compute precision at K\n",
    "    for doc_id in results_considered:\n",
    "        if doc_id in ground_truth_set: relevant_documents_retrieved += 1\n",
    "    precision = relevant_documents_retrieved / len(results_considered) if results_considered else 0\n",
    "\n",
    "    # Compute recall at K\n",
    "    total_relevant = len(ground_truth_set)\n",
    "    recall = relevant_documents_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if precision + recall == 0: return 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 F1Score@5: 0.4000\n",
      "Query 2 F1Score@10: 0.6400\n",
      "Query 2 F1Score@15: 0.8667\n",
      "Query 2 F1Score@17: 0.9375\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_1_results)])))\n",
    "for K in K_values:\n",
    "    f1 = f1_score_at_k(query_1_relevant, query_1_results, K)\n",
    "    print(f\"Query 2 F1Score@{K}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 F1Score@5: 0.5000\n",
      "Query 2 F1Score@10: 0.8000\n",
      "Query 2 F1Score@15: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_2_results)])))\n",
    "for K in K_values:\n",
    "    f1 = f1_score_at_k(query_2_relevant, query_2_results, K)\n",
    "    print(f\"Query 2 F1Score@{K}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Query 1 - India protest             - F1Score@5  : 0.6667\n",
      "Personalized Query 1 - India protest             - F1Score@10 : 0.6667\n",
      "Personalized Query 1 - India protest             - F1Score@15 : 0.6667\n",
      "\n",
      "Personalized Query 2 - support farmers           - F1Score@5  : 0.6667\n",
      "Personalized Query 2 - support farmers           - F1Score@10 : 1.0000\n",
      "Personalized Query 2 - support farmers           - F1Score@15 : 1.0000\n",
      "\n",
      "Personalized Query 3 - Modi shame                - F1Score@5  : 0.6667\n",
      "Personalized Query 3 - Modi shame                - F1Score@10 : 1.0000\n",
      "Personalized Query 3 - Modi shame                - F1Score@15 : 1.0000\n",
      "\n",
      "Personalized Query 4 - BJP party                 - F1Score@5  : 0.7143\n",
      "Personalized Query 4 - BJP party                 - F1Score@9  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - F1Score@10 : 1.0000\n",
      "Personalized Query 4 - BJP party                 - F1Score@15 : 1.0000\n",
      "\n",
      "Personalized Query 5 - human rights violated     - F1Score@5  : 0.6667\n",
      "Personalized Query 5 - human rights violated     - F1Score@10 : 1.0000\n",
      "Personalized Query 5 - human rights violated     - F1Score@15 : 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, query_text in enumerate(query_texts, start=1):\n",
    "    relevant_docs = personalized_query_relevant[i]\n",
    "    results = personalized_query_results[i]\n",
    "    \n",
    "    K_values = list(sorted(set([5, 10, 15, len(results)])))\n",
    "    for K in K_values:\n",
    "        f1 = f1_score_at_k(relevant_docs, results, K)\n",
    "        print(f\"Personalized Query {i} - {query_text:<25} - F1Score@{K:<3}: {f1:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision_at_k(queries_ground_truth, queries_results, K=10):\n",
    "    ap_scores = []\n",
    "    for ground_truth, results in zip(queries_ground_truth, queries_results):\n",
    "        ap = average_precision_at_k(ground_truth, results, K=K)\n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    if ap_scores: return sum(ap_scores) / len(ap_scores)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_ground_truth = (query_1_relevant, query_2_relevant)\n",
    "queries_results = (query_1_results, query_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 & 2 MAP@5: 0.3000\n",
      "Query 1 & 2 MAP@10: 0.5681\n",
      "Query 1 & 2 MAP@15: 0.8755\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, min(len(query_2_results), len(query_2_results))])))\n",
    "for K in K_values:\n",
    "    precision = mean_average_precision_at_k(queries_ground_truth, queries_results, K)\n",
    "    print(f\"Query 1 & 2 MAP@{K}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Queries MAP@5: 0.5111\n",
      "Personalized Queries MAP@10: 0.9000\n",
      "Personalized Queries MAP@15: 0.9000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15])))\n",
    "for K in K_values:\n",
    "    precision = mean_average_precision_at_k(list(personalized_query_relevant.values()), list(personalized_query_results.values()), K)\n",
    "    print(f\"Personalized Queries MAP@{K}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(queries_results, queries_ground_truth):\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for results, ground_truth in zip(queries_results, queries_ground_truth):\n",
    "        ground_truth_set = set(ground_truth)\n",
    "        reciprocal_rank = 0\n",
    "        for rank, doc_id in enumerate(results, start=1):\n",
    "            if doc_id in ground_truth_set:\n",
    "                reciprocal_rank = 1 / rank\n",
    "                break\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "    \n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 & 2 MRR: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query 1 & 2 MRR: {mean_reciprocal_rank(queries_ground_truth, queries_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Queries MRR: 0.8400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Personalized Queries MRR: {mean_reciprocal_rank(list(personalized_query_relevant.values()), list(personalized_query_results.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevances, k, method=1):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size:\n",
    "        if method == 1:  # Standard method\n",
    "            return relevances[0] + np.sum(relevances[1:] / np.log2(np.arange(2, k + 1)))\n",
    "        elif method == 2:  # Alternative method\n",
    "            return np.sum((2**relevances - 1) / np.log(np.arange(1, k + 1) + 1))\n",
    "    return 0\n",
    "\n",
    "def ndcg_at_k(ground_truth, results, k, method=1):\n",
    "    # assert k <= len(results)\n",
    "    if k > len(results): \n",
    "        warnings.warn(\"k is greater than the number of results. Adjusting to maximum available.\")\n",
    "        k = min(k, len(results))\n",
    "    \n",
    "    ground_truth_set = set(ground_truth)\n",
    "    # Get binary relevance for the actual results\n",
    "    actual_relevance = [1 if doc_id in ground_truth_set else 0 for doc_id in results[:k]]\n",
    "    \n",
    "    # Compute DCG for actual results\n",
    "    actual_dcg = dcg_at_k(actual_relevance, k, method)\n",
    "    \n",
    "    # Sort the binary relevance to compute ideal DCG\n",
    "    ideal_relevance = sorted(actual_relevance, reverse=True)\n",
    "    ideal_dcg = dcg_at_k(ideal_relevance, k, method)\n",
    "    \n",
    "    if ideal_dcg == 0: return 0 \n",
    "    return actual_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 NDCG@5: 1.0000\n",
      "Query 1 NDCG@10: 0.9567\n",
      "Query 1 NDCG@15: 0.9509\n",
      "Query 1 NDCG@17: 0.9512\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_1_results)])))\n",
    "for K in K_values:\n",
    "    ndcg = ndcg_at_k(query_1_relevant, query_1_results, K)\n",
    "    print(f\"Query 1 NDCG@{K}: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 NDCG@5: 1.0000\n",
      "Query 2 NDCG@10: 1.0000\n",
      "Query 2 NDCG@15: 1.0000\n"
     ]
    }
   ],
   "source": [
    "K_values = list(sorted(set([5, 10, 15, len(query_2_results)])))\n",
    "for K in K_values:\n",
    "    ndcg = ndcg_at_k(query_2_relevant, query_2_results, K)\n",
    "    print(f\"Query 2 NDCG@{K}: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Query 1 - India protest             - NDCG@5  : 1.0000\n",
      "Personalized Query 1 - India protest             - NDCG@10 : 1.0000\n",
      "Personalized Query 1 - India protest             - NDCG@15 : 1.0000\n",
      "\n",
      "Personalized Query 2 - support farmers           - NDCG@5  : 1.0000\n",
      "Personalized Query 2 - support farmers           - NDCG@10 : 1.0000\n",
      "Personalized Query 2 - support farmers           - NDCG@15 : 1.0000\n",
      "\n",
      "Personalized Query 3 - Modi shame                - NDCG@5  : 1.0000\n",
      "Personalized Query 3 - Modi shame                - NDCG@10 : 1.0000\n",
      "Personalized Query 3 - Modi shame                - NDCG@15 : 1.0000\n",
      "\n",
      "Personalized Query 4 - BJP party                 - NDCG@5  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - NDCG@9  : 1.0000\n",
      "Personalized Query 4 - BJP party                 - NDCG@10 : 1.0000\n",
      "Personalized Query 4 - BJP party                 - NDCG@15 : 1.0000\n",
      "\n",
      "Personalized Query 5 - human rights violated     - NDCG@5  : 1.0000\n",
      "Personalized Query 5 - human rights violated     - NDCG@10 : 1.0000\n",
      "Personalized Query 5 - human rights violated     - NDCG@15 : 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t1/rkwffv314k333khdlgxlw40m0000gn/T/ipykernel_26629/3727027718.py:13: UserWarning: k is greater than the number of results. Adjusting to maximum available.\n",
      "  warnings.warn(\"k is greater than the number of results. Adjusting to maximum available.\")\n"
     ]
    }
   ],
   "source": [
    "for i, query_text in enumerate(query_texts, start=1):\n",
    "    relevant_docs = personalized_query_relevant[i]\n",
    "    results = personalized_query_results[i]\n",
    "    \n",
    "    K_values = list(sorted(set([5, 10, 15, len(results)])))\n",
    "    for K in K_values:\n",
    "        ndcg = ndcg_at_k(relevant_docs, results, K)\n",
    "        print(f\"Personalized Query {i} - {query_text:<25} - NDCG@{K:<3}: {ndcg:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Representation using T-distributed Stochastic Neighbor Embedding (T-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample data: Replace this with your actual dataframe column\n",
    "tweets = [\"This is a sample tweet\", \"Another tweet for analysis\", \"Machine learning with Python\", \"Text data visualization\"]\n",
    "\n",
    "# Create TF-IDF model\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_matrix = vectorizer.fit_transform(tweets)\n",
    "\n",
    "# tfidf_matrix is a sparse matrix of shape (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuyi/Desktop/IRWA/IRWA/venv/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a TSNE model: set perplexity and n_iter according to your dataset size\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tsne_model \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tsne_results \u001b[38;5;241m=\u001b[39m \u001b[43mtsne_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# convert to array if necessary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# tsne_results now holds the 2D coordinates of your tweets\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/IRWA/IRWA/venv/lib/python3.9/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/IRWA/IRWA/venv/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IRWA/IRWA/venv/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1175\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/Desktop/IRWA/IRWA/venv/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:864\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 864\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a TSNE model: set perplexity and n_iter according to your dataset size\n",
    "tsne_model = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne_model.fit_transform(tfidf_matrix.toarray())  # convert to array if necessary\n",
    "\n",
    "# tsne_results now holds the 2D coordinates of your tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], edgecolor='k', alpha=0.5)\n",
    "plt.title('Tweet Visualization using T-SNE')\n",
    "plt.xlabel('TSNE Component 1')\n",
    "plt.ylabel('TSNE Component 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
